\paragraph{}
The filter \(W\) is built on top of the kernel matrix \(K\) measuring the similarity between each pixel.
The most popular kernel functions are the \textit{bilateral filter} \cite{bilateral_tomasi_1998} and the \textit{non-local mean filter} \cite{kervrann_nlm_2006} to measure these similarities.
The kernel functions create a symmetric positive semi-definite (SPSD) matrix \(K\), so the eigenvalues of \(K\) are non-negative, \(\lambda_K \ge 0\), as described in \cite{talebi_fast_2016}.
Also, from the definition of the filters, \(\forall i, j \in [1, N]\), \(N\) the number of pixels, the values of the affinity matrix \(K\) are non-negative \(K_{ij} \ge 0\).

\paragraph{}
Without loss of generality, we shall use a Laplacian operator that is symmetric positive definite (SPD) by definition.
The filter is implicitly defined by \cite{modern_tour_2013} as \(W = I - \Lapl\) and if the Laplacian eigenvalues are between 0 and 1, so are the eigenvalues of \(W\).

We can say that for a principal submatrix \(W_A\) of size \(p \times p\) of the matrix \(W\) of size \(N \times N\) with \(p \ll N\), the eigenvalues \(\lambda^W_i \le \lambda^{W_A}_i \le \lambda^W_{i+N-p}\).
This is the interlacing property, meaning in general, \(\lambda^W_1 \le \lambda^{W_A}_i \le \lambda^W_N\).

\paragraph{Proof of interlacing}
Let \(A\) be a symmetric matrix of size \(n\), \(\lambda^A_n\) be the largest eigenvalue of \(A\) and \(\lambda^A_1\) the smallest one.
Let \(R\) be the restriction operator, such as, with \(u\) a non-zero vector, \(Ru = \begin{pmatrix}\alpha_1 \\ \alpha_2 \\ \vdots \\ 0 \\ \vdots \\ 0 \end{pmatrix}\) for example.
This defines \(RAR^T\) a \(s \times s\) principal submatrix of \(A\) withÂ  \(s \in [1; n]\).
Suppose the remaining rows and columns of \(A\) in \(RAR^T\) are indexed by \(S\) of size \(s\). \\
Let \(\mathcal{U} \in \Real^s\) and \(u \in \Real^n\) with \(\begin{cases} u_i = \mathcal{U}_i & \quad \text{if } i \in S \\ u_i = 0 & \quad \text{if } i \notin S \end{cases}\).
Given a \(k \in [1; s]\), the Courant-Fischer theorem, involving the Rayleigh-Ritz quotient, implies that,
\[max\left(\frac{\langle Au, u \rangle}{\langle u, u\rangle}\right) = max\left(\frac{\langle RAR^T\mathcal{U}, \mathcal{U}\rangle}{\langle \mathcal{U}, \mathcal{U} \rangle}\right) \ge \lambda^A_k.\]
So \(\lambda^{RAR^T}_k \ge \lambda^A_k\).
Moreover, in the other way, we get
\[min\left(\frac{\langle Au, u \rangle}{\langle u, u\rangle}\right) = min\left(\frac{\langle RAR^T\mathcal{U}, \mathcal{U}\rangle}{\langle \mathcal{U}, \mathcal{U} \rangle}\right) \le \lambda^A_{k+n-s}.\]
And so again, \(\lambda^{RAR^T}_k \le \lambda^A_{k+n-s}\).
This concludes the proof, showing that the eigenvalues of the submatrix are bounded by the eigenvalues of the original matrix.
More precisely, we proved the interlacing property of the eigenvalues of \(RAR^T\) such as
\[\lambda^A_k \le \lambda^{RAR^T}_k \le \lambda^A_{k+n-s}.\]

\paragraph{}
From the definition of the filter \(W = I - \Lapl\), we have the submatrix \(W_A = I - \Lapl_A\), with \(I\) being the identity of appropriate order.
For the algorithm, we need to compute the largest eigenvalues of \(W_A\).

\paragraph{Theorem}
Computing the largest eigenvalues of \(W_A\) is equivalent to computing the smallest eigenvalues of \(\Lapl_A\).

\paragraph{Proof}
Let \(\lambda\) be an eigenvalue of \(W_A\) and \(x\) the associate eigenvector:
\begin{equation}
 \begin{split}
     W_A x = \lambda x & \Leftrightarrow (I - \Lapl_A)x = \lambda x \\
                     & \Leftrightarrow x - \Lapl_A x = \lambda x \\
                     & \Leftrightarrow \Lapl_A x = x - \lambda x \\
                     & \Leftrightarrow \Lapl_A x = (1 - \lambda) x
 \end{split}
\end{equation}
So the eigenvalues of the Laplacian submatrix \(\mu = 1 - \lambda\).
We can thus get the greatest eigenvalues of \(W_A\) by computing the smallest eigenvalues of \(\Lapl_A\).

\paragraph{Speed of convergence}
For both these problems, finding the greatest and smallest eigenvalues, the most famous methods are, respectively, the power method and inverse power method\footnote{Those are also called power iteration and inverse iteration. The inverse method has a variant called inverse subspace iteration, to find the associated subspace to the eigenvalues.}.

For the power iteration, the convergence rate is \(|\frac{\lambda_2}{\lambda_1}|\), with \(\lambda_2\) being the second largest eigenvalue.
We know that \(\lambda^{W_A}_2 \le \lambda^{W_A}_1 \le 1\) and thus \(\frac{\lambda^{W_A}_2}{\lambda^{W_A}_1} \le \frac{\lambda^{W_A}_1}{\lambda^{W_A}_1} = 1\).
The convergence rate is lower than 1.
The method is fast if the rate is small and slow if the rate is close to 1.
So the closer the two eigenvalues are, the slower the method converges.

The inverse iteration has a speed of convergence of \(|\frac{\mu_1}{\mu_2}|\), with \(\mu_2\) the second smallest eigenvalue.
Again, we know that \(0 \le \mu^{\Lapl_A}_1 \le \mu^{\Lapl_A}_2\).
So the convergence speed is also lower than 1.

We come to the conclusion that both methods depend on the spacing between the eigenvalues.
The closer they are, the more iterations will be required to converge.
The difference of convergence speeds for both methods therefore depends on the distance between the largest eigenvalues and the distance between the smallest ones.

\paragraph{}
Inverse iterations implies either to compute the inverse of the matrix \(x_{k+1} = A^{-1}x_k\), or to solve a system of linear equations \(Ax_{k+1} = x_k\).
Since the image processing context suggests having dense matrices, we want to explore the performances of Krylov methods and domain decomposition methods (e.g. the Additive Schwarz method) on such dense matrices.
