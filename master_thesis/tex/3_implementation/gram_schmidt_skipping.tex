Fundamentally, the orthogonalisation is used to stabilise the algorithm.
\ifthesis
 To accelerate our algorithm further, we try to orthogonalise the vectors \(X_k\) every other iteration instead of every iteration.
 We present below the resulting performances:

 \begin{figure}[H]
  \centering
  \input{tex/plots/inv_it_runtime_50_and_500_gs}
  \caption{Runtime of the inverse subspace iteration with skipping the Gram-Schmidt procedure every other iteration (log scale).}
 \end{figure}

 The performances for 50 eigenvalues are similar to the case when we are not skipping the Gram-Schmidt orthogonalisation every other iteration.
 We saw that only a small proportion of time is spent doing the orthogonalisation in this case, so the impact is not significant.

 However, for computing 500 eigenvalues, the runtime with skipping the orthogonalisation every other iteration is much lower.
 Most time is spent doing the Gram-Schmidt process, so the execution is considerably sped up.
 For 2 processors, the runtime is around 5600 seconds and the fastest runtime is 2100 seconds for 64 processors.
 Even if the algorithm requires a few more outer iterations to converge, we nearly observe a factor 2 speed up with respect to the algorithm without skipping the Gram-Schmidt procedure.
 The communication overhead of the method remains a problem when the number of processors is large.

 When skipping the Gram-Schmidt more often, we might see further improvements.
\else
 To accelerate our algorithm further, we try to orthogonalise the vectors \(X_k\) less often and skip some Gram-Schmidt procedures.
\fi

Below the runtime for 2 and 64 processors of the inverse subspace algorithm depending on the frequency of orthogonalisation:

\begin{figure}[H]
 \centering
 \input{tex/plots/skip_gs}
 \caption{Runtime of the inverse subspace iteration depending on the amount of Gram-Schmidt procedures.}
 \label{fig:skip_gs}
\end{figure}

For 2 processors, we see in figure \ref{fig:skip_gs} that the speedup stagnates quickly when skipping the Gram-Schmidt orthogonalisation more and more often.
Indeed, the orthogonalisation represents approximately a quarter of the execution time for 2 processors, whereas it represents over 85\% for 64 processors.
We observe from figure \ref{fig:skip_gs} a speedup of the inverse subspace iteration that is nearly linear for 64 processors.
The less we stabilise the algorithm through orthogonalisation, the faster it gets in this case. 
The number of outer iterations between not skipping the Gram-Schmidt algorithm and applying it every 8 iterations only varies from 38 to 42 which explains the resulting runtime.

Skipping the limiting Gram-Schmidt procedure shows good results for the runtime.
We can now observe the runtime of the state-of-the-art parallel algorithm provided by the SLEPc library.
