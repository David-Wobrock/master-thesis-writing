As a reminder, we used GMRES to solve the linear systems with RAS preconditioning and using GMRES on the subdomains.
We sample 1\% of the pixels of a image with \(4 \cdot 10^5\) pixels using spatially uniform sampling.
The performances of the inverse subspace iteration for 50 and 500 eigenvalues:

\begin{figure}[H]
  \centering
  \input{tex/plots/inv_it_runtime_50_and_500}
  \caption{Runtime of the inverse subspace iteration part of the algorithm (log scale).}
\end{figure}

When increasing a low number of processors, we see an improvement of the performances in both cases.
But the runtime stagnates slowly for 50 eigenvalues and quickly for 500 eigenvalues.
We even observe a raise of the runtime for 500 eigenvalues.
The algorithm reaches its parallelisation limit and the communication overhead takes over.
For 500 eigenvalues, the runtime for 2 processors is over 7000 seconds, and the fastest runtime is reached for 64 processors and is of 3700 seconds.

We know that the inverse iteration part of the algorithm is not scaling correctly compared to the other parts.
For any amount of computed eigenvalues, when we increase the number of processes, the proportion of time spent computing the eigenvalues increases.
For 500 eigenvalues and 128 processors, we spend more than 99\% of the time computing the eigenvalues.
This confirms that the algorithm does not quite scale yet.

We look at the internal steps of the inverse power method to see where lies the problem.
The algorithm consists of iteratively solving \(m\) linear systems, orthonormalising the vectors and computing the residual norm.
Here is the proportion of each step of the inverse subspace interation for the computation of 50 and 500 eigenvalues:

\begin{figure}[H]
  \centering
  \input{tex/plots/inv_it_proportion_50_and_500}
  \caption{Proportion of each step in the inverse subspace iteration.}
\end{figure}

We observe that the Gram-Schmidt orthogonalisation is the limiting factor and is the most time-consuming step of the inverse iteration as the number of processors grows.
It is a well-known problem that the simple Gram-Schmidt process is actually difficult to parallelise efficiently.
Small optimisations for a parallel Gram-Schmidt orthogonalisation exist \cite{katagiri_parallel_gram_schmidt_2003} but they do not properly solve the problem.
This issue will be difficult to overcome completely.
