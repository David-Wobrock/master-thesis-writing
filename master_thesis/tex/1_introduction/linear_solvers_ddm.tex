\paragraph{}
Solving a system of linear equations such that
\[Ax = b,\]
is often critical in scientific computing.
When discretising equations coming from physics for example, a huge linear system can be obtained.
Multiple methods exist to solve such systems, even when the system is large and expensive to compute.
We present in the following the most used and known solvers.

\paragraph{Direct solvers}
The most commonly used solvers for systems of linear equations are direct solvers.
They provide robust methods and optimal solutions to the problem.
However, they can be hard to parallelise and have difficulties with large input.
The most famous is the backslash operator from MATLAB which performs tests to determine which special case algorithm to use, but ultimately falls back on a LU factorisation \cite{mldivide_matlab}.
The LU factorisation, which is a Gaussian elimination procedure, is hard to parallelise.
Although, a block version of the LU factorisation exists that can be parallelised more easily.
There are other parallel direct solvers, like MUMPS \cite{MUMPS_2001}, but generally they reach their computational limit above \(10^6\) degrees of freedom in a 2D problem, and \(10^5\) in 3D.

\paragraph{Iterative solvers}
For larger problems, iterative methods must be used to achieve reasonable runtime performances.
The two types of iterative solvers are fixed-point iteration methods and Krylov type methods.
Both require only a small amount of memory and can often be parallelised.
The main drawback is that these methods tend to be less robust than direct solvers and convergence depends on the problem.
Indeed, ill-conditioned input matrices will be difficult to solve correctly by iterative methods and do not necessarily converge.
Generally, Krylov methods are preferred over fixed-point iteration methods.
The most relevant iterative Krylov methods are the Conjugate Gradient (CG) and the Generalised Minimal Residual method (GMRES) \cite{saad_iterative_2003} \cite{saad_gmres_1986}.

To tackle the ill-conditioned matrices problem, there is a need to precondition the initial system.


\paragraph{Preconditioners - Domain decomposition methods}
One of the ways to precondition systems of linear equations is to use domain decomposition.
The idea goes back to Schwarz who wanted to solve a Poisson problem on a complex geometry.
He decomposed the geometry into multiple smaller simple geometric forms, making it easy to work on subproblems.
This idea has been extended and improved to propose fixed-point iterations solvers for linear systems.
However, Krylov methods expose better results and faster convergence, but domain decomposition methods can actually be used as preconditioners to the system.
Famous Schwarz preconditioners are the Restricted Additive Schwarz method (RAS) and the Additive Schwarz Method (ASM).
With \(M^{-1}\) the preconditioning matrix, we shall solve
\[M^{-1}Ax = M^{-1}b\]
which exposes the same solution as the original problem.

\paragraph{}
Domain decomposition methods will also be an important topic of this degree project.
These methods are usually applied to solve problems of linear algebra involving partial differential equations.
Solving the discretised problem leads to solving linear systems.

Our main reference will be \cite{dolean_domain_2015} which focuses on the parallel linear iterative solvers for systems of linear equations.
Domain decomposition methods are naturally parallel which is convenient for the current state of processor progress.
Without going into the details, we will make use of Schwarz methods for preconditioning and iterative Krylov subspace methods as solvers.
