\paragraph{Background}
Even with high quality cameras, denoising and improving a taken picture remains important.
The two main issues that have to be addressed by denoising are blur and noise.
The effect of blur is internal to cameras since the number of samples of the continuous signal is limited and it should hold the Shannon-Nyquist theorem \cite{buades_review_2005}, stipulating a sufficient condition on the number of samples required to discretise a continuous signal without losing information.
Noise comes from the light acquisition system that fluctuates in relation to the amount of incoming photons.

To model these problems, a classical approach to formulate the deficient image considers \(z\) the clean signal vector, \(e\) a noise vector of variance \(\sigma^2\) and \(y\) the noisy picture:
\[y = z + e.\]

What we want is a high-performance denoiser, capable of scaling up in relation to increasing the image size and keeping reasonable performances.
The output image should come as close as possible to the clean image.
As an important matter, it is now generally accepted that images contain a lot of redundancy.
This means that, in a natural image, every small enough window has many similar windows in the same image.

\paragraph{Traditional, patch-based methods}
The image denoising algorithms review proposed by \cite{buades_review_2005} suggests that the non-local means algorithm, compared to other reviewed methods, comes closest to the original image when applied to a noisy image.
This algorithm takes advantage of the redundancy of natural images and for a given pixel predicts its value by using the pixels in its neighbourhood.

In \cite{dabov_image_2007}, the authors propose the BM3D algorithm, a denoising strategy based on grouping similar 2D blocks of the image into 3D data arrays.
Then, collaborative filtering is performed on these groups and return 2D estimates of all grouped blocks.
This algorithm exposed state-of-the-art performance in terms of denoising at that time.
The results are still one of the best for a reasonable computational cost.

\paragraph{Global filter}
In the last couple of years, global image denoising filters came up, based on spectral decomposition \cite{glide_2014}.
This approach considers the image as a complete graph, where the filter value of each pixel is computed by all pixels in the image.
We define the result image \(z\), \(W\) the huge data-dependent global filter of size \(N \times N\), with \(N\) the number of pixels and the input image \(y\):
\[z = Wy.\]
To show an example of the size of the filter, a standard 10 MPixel picture will result in a filter matrix of \(10^{14}\) elements, taking 800 TB of memory.
This kind of filter is considered in this report.

Those huge matrices will need to be approximated using their eigendecomposition.
And these will need to solve systems of linear equations.
