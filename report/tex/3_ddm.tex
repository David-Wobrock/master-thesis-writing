\chapter{Linear Systems and Domain Decomposition Methods}

\input{tex/3_1_eigenvalue_theory}

\section{Theoretical basis}

\paragraph{}
Solving a system of linear equations such that
\[Ax = b,\]
is often critical in scientific computing.
When discretising equations coming from physics for example, a huge linear system can be obtained.
Multiple methods exist to solve such systems, even when the system is large and expensive to compute.
We present in the following the most used and known solvers.

\subsection{Direct solvers}

\paragraph{}
The most commonly used solvers for systems of linear equations are direct solvers.
They provide robust methods and optimal solutions to the problem.
However, they can be hard to parallelise and have difficulties with large input.
The most famous is the backslash operator from MATLAB which performs tests to determine which special case algorithm to use, but ultimately falls back on a LU factorisation \cite{mldivide_matlab}.
The LU factorisation, closely related to Gaussian elimination, is hard to parallelise.
A block version of the LU factorisation exists that can be parallelised.
Other direct solvers, like MUMPS, exist and can be used, but generally they reach a computational limit above \(10^6\) degrees of freedom in a 2D problem, and \(10^5\) in 3D.

\subsection{Iterative solvers}

\paragraph{}
For large problems, iterative methods must be used to achieve a reasonable running time.
The two types of iterative solvers are fixed-point iteration methods and Krylov type methods.
Both require only a small amount of memory and can often be parallelised.
The main drawback is that these methods tend to be less robust than direct solvers and convergence depends on the problem.
Indeed, ill-conditioned input matrices will be difficult to solve correctly by iterative methods.
The most relevant iterative methods are the conjugate gradient and GMRES \cite{saad_gmres_1986}.

To tackle the ill-conditioned matrices problem, there is a need to precondition the system.

\subsection{Domain decomposition methods}

\paragraph{}
One of the ways to precondition systems of linear equations is to use domain decomposition.
The idea goes back to Schwarz who wanted to solve a Poisson problem on a complex geometry.
He decomposed the geometry into multiple smaller simple geometric forms, making it easy to work on subproblems.
This idea has been extended and improved to propose fixed-point iterations solvers for linear systems.
However, Krylov methods expose better results and faster convergence, but domain decomposition methods can actually be used as preconditioners to the system.
The most famous Schwarz preconditioners are the Restricted Additive Schwarz (RAS) and Additive Schwarz Method (ASM).
For example, the formulation of the ASM preconditioning matrix
\[M^{-1}_{ASM} = \sum_i R_i^T A_i^{-1} R_i,\]
with \(i\) subdomains and \(R_i\) the restriction matrix of \(A\) to the \(i\)-th subdomain.
With such a preconditioner we will be able to solve
\[M^{-1}Ax = M^{-1}b\]
which exposes the same solution as the original problem.

\section{Motivation and tools}

\paragraph{}
In the case of our image processing algorithm, choosing a sufficient number of sample pixels is essential for a good approximation of the matrices eigenvalues and eigenvectors.
As exposed in the litterature \cite{glide_2014} \cite{fowlkes_spectral_2004}, less than 1\% of the pixels seems to be enough to capture most of the image information.
However, applied to very high resolution images, 1\% of the number of pixels is still a large amount and causes to compute large dense matrices.
Additionally, we intend to apply this algorithm on 3D images where the problem size grows even further.

\paragraph{}
As exposed previously, the algorithm contains matrix computations and eigenvalue problems.
The computations are mostly independent (row independent for most matrix computations).
Therefore, there is an opportunity and a need to speed up the algorithm by parallalising it on supercomputers.
For this, we use the PETSc library (Portable, Extensible Toolkit for Scientific Computation) \cite{petsc_web_page}, which makes use of HPC tools like the MPI standard to distribute and compute efficiently matrices and vectors.
Furthermore, the library SLEPc (Scalable Library for Eigenvalue Problem Computations) \cite{hernandez_2005_slepc}, based on PETSc, is used to solve the eigenvalue problems efficiently.
